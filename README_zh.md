<div align="center">
<img src="assets/mPLUG_new1.png" width="80%">
</div>

# mPLUG-OwlğŸ¦‰: Modularization Empowers Large Language Models with Multimodality
<div align="center">
Qinghao Ye*, Haiyang Xu*, Guohai Xu*, Jiabo Ye, Ming Yanâ€ , Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang
</div>

<div align="center">
<strong>é˜¿é‡Œå·´å·´é›†å›¢ï¼Œè¾¾æ‘©é™¢</strong>
</div>

<div align="center">
*Equal Contribution; â€  Corresponding Author
</div>

<div align="center">
    <a href="https://huggingface.co/spaces/MAGAer13/mPLUG-Owl"><img src="https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm-dark.svg" alt="Open in Spaces"></a>
    <a href="https://modelscope.cn/studios/damo/mPLUG-Owl/summary"><img src="assets/Demo-ModelScope-brightgreen.svg" alt="Demo ModelScope"></a>
    <a href="https://github.com/X-PLUG/mPLUG-Owl/blob/main/LICENSE"><img src="assets/LICENSE-Apache%20License-blue.svg" alt="License"></a>
    <a href="http://mm-chatgpt.oss-cn-zhangjiakou.aliyuncs.com/mplug_owl_demo/released_checkpoint/mPLUG_Owl_paper.pdf"><img src="assets/Paper-PDF-orange.svg"></a>
    <a href="https://arxiv.org/abs/2304.14178"><img src="assets/Paper-Arxiv-orange.svg" ></a>
    <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FX-PLUG%2FmPLUG-Owl&count_bg=%23E97EBA&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=visitors&edge_flat=false" alt="Hits"></a>
    <a href="https://twitter.com/xuhaiya2483846/status/1654640739010351106"><img src='assets/-twitter-blue.svg'></a>
</div>

<div align="center">
<a href="README.md">English</a> | <a>ç®€ä½“ä¸­æ–‡</a>
<hr>
</div>

<!--
[![Open in Spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm-dark.svg)](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl)
[![](assets/Demo-ModelScope-brightgreen.svg)](https://modelscope.cn/studios/damo/mPLUG-Owl/summary)
[![](assets/LICENSE-Apache%20License-blue.svg)](https://github.com/X-PLUG/mPLUG-Owl/blob/main/LICENSE)
[![](assets/Paper-PDF-orange.svg)](http://mm-chatgpt.oss-cn-zhangjiakou.aliyuncs.com/mplug_owl_demo/released_checkpoint/mPLUG_Owl_paper.pdf)
[![](assets/Paper-Arxiv-orange.svg)](https://arxiv.org/abs/2304.14178)
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FX-PLUG%2FmPLUG-Owl&count_bg=%23E97EBA&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=visitors&edge_flat=false)](https://hits.seeyoufarm.com)

[English](README.md) | ç®€ä½“ä¸­æ–‡
<hr>
-->
<div align="center">
<img src="http://mm-chatgpt.oss-cn-zhangjiakou.aliyuncs.com/mplug_owl_demo/released_checkpoint/sample.gif"  width="60%">
</div>

## ç¤ºä¾‹
![Training paradigm and model overview](assets/case_1.png "Training paradigm and model overview")
![Training paradigm and model overview](assets/case_2.png "Training paradigm and model overview")

## æœ€æ–°æ›´æ–°
* ğŸ”¥ [05.30] **å¤šè¯­è¨€ç‰ˆæœ¬**çš„æƒé‡ç›®å‰å·²ç»å‘å¸ƒåœ¨[Huggingface Model Hub](https://huggingface.co/MAGAer13/mplug-owl-bloomz-7b-multilingual).
* ğŸ”¥ [05.27] æˆ‘ä»¬ç°åœ¨åœ¨[ModelScope](https://www.modelscope.cn/studios/damo/mPLUG-Owl-Bilingual/summary)ä¸Šæä¾›äº†ä¸€ä¸ªå¤šè¯­è¨€ç‰ˆæœ¬çš„mPLUG-Owlï¼ŒåŒæ—¶ä¹Ÿæä¾›äº†checkpointã€‚
* ğŸ”¥ [05.19] mPLUG-Owlç°åœ¨*åŸç”Ÿæ”¯æŒ Huggingface*çš„ç”¨æ³•å’Œæ”¯æŒHuggingface Trainerè®­ç»ƒï¼Œä»…éœ€*1å¼ 32Gçš„V100*å³å¯å¼€å¯è®­ç»ƒ! æˆ‘ä»¬é‡æ„äº†ä»£ç ç§»é™¤äº†Apexçš„ä¾èµ–. ç¦»çº¿Demoæ”¯æŒ*8æ¯”ç‰¹*è¿›è¡Œæ¨ç†ï¼Œä»…éœ€è¦1å¼ *16GB T4*å³å¯éƒ¨ç½²! 
* [05.16] æˆ‘ä»¬åŸºäºè§†é¢‘å›¾æ–‡æ•°æ®è”åˆè®­ç»ƒäº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œåœ¨çº¿demoå·²ç»æ›´æ–°ã€‚æ›´æ–°åçš„checkpointå’Œä»£ç ä¼šå¾ˆå¿«å’Œå¤§å®¶è§é¢ï¼
* [05.16] [HuggingFace](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl) ä¸Šçš„åœ¨çº¿demoç°åœ¨æ”¯æŒ8bitäº†ï¼
* [05.12] åœ¨çº¿ demo å’Œ API å·²ç»å¼€æ”¾åœ¨[Replicate](https://replicate.com/joehoover/mplug-owl)!
* [05.05] æˆ‘ä»¬å‘å¸ƒäº†æŒ‡ä»¤å¾®è°ƒçš„ä»£ç ã€‚
* [05.05] æˆ‘ä»¬åœ¨[HuggingFace](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl)ä¸Šä¹Ÿæ­å»ºäº†Demoã€‚æ„Ÿè°¢HuggingFaceæä¾›çš„å…è´¹ç®—åŠ›ï¼
* [05.05] HuggingFaceä¸Šçš„Demoç°åœ¨å·²ç»æ”¯æŒè§†é¢‘è¾“å…¥ï¼ModelScopeä¸Šçš„Demoä¹Ÿå³å°†æ”¯æŒã€‚
* [05.05] æˆ‘ä»¬å…¬å¼€äº†è§†è§‰ç›¸å…³æŒ‡ä»¤çš„æµ‹è¯„é›†**OwlEval**
* [04.26] æˆ‘ä»¬åœ¨Modelscopeä¸Šæä¾›äº†ä¸€ä¸ª[åœ¨çº¿Demo](https://modelscope.cn/studios/damo/mPLUG-Owl/summary)ä¾›å¤§å®¶ä½“éªŒã€‚
* [04.26] æˆ‘ä»¬å¼€æ”¾äº†mPLUG-OwlğŸ¦‰ï¼Œä»¥åŠæ¨ç†ä»£ç å’ŒäºŒé˜¶æ®µå¾®è°ƒå‚æ•°ã€‚

## äº®ç‚¹ç‰¹è‰²
* ä¸€ç§é¢å‘å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„**æ¨¡å—åŒ–**çš„è®­ç»ƒèŒƒå¼ã€‚
* èƒ½å­¦ä¹ ä¸è¯­è¨€ç©ºé—´ç›¸é€‚åº”çš„è§†è§‰çŸ¥è¯†ï¼Œå¹¶æ”¯æŒåœ¨å¤šæ¨¡æ€åœºæ™¯(æ”¯æŒå›¾ç‰‡ã€è§†é¢‘ã€æ–‡æœ¬è¾“å…¥)ä¸‹è¿›è¡Œ**å¤šè½®å¯¹è¯**ã€‚
* æ¶Œç°**å¤šå›¾å…³ç³»ç†è§£**ï¼Œ**åœºæ™¯æ–‡æœ¬ç†è§£**å’Œ**åŸºäºè§†è§‰çš„æ–‡æ¡£ç†è§£**ç­‰èƒ½åŠ›ã€‚
* æå‡ºäº†é’ˆå¯¹è§†è§‰ç›¸å…³æŒ‡ä»¤çš„æµ‹è¯„é›†**OwlEval**ï¼Œç”¨ä»¥è¯„ä¼°å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹çš„å¯¹å¸¦æœ‰è§†è§‰ä¿¡æ¯ä¸Šä¸‹æ–‡çš„ç†è§£èƒ½åŠ›ã€‚
* æˆ‘ä»¬åœ¨æ¨¡å—åŒ–ä¸Šçš„ä¸€äº›æ¢ç´¢:
  * [E2E-VLP](https://aclanthology.org/2021.acl-long.42/), [mPLUG](https://aclanthology.org/2022.emnlp-main.488/) å’Œ [mPLUG-2](https://arxiv.org/abs/2302.00402), åˆ†åˆ«è¢«ACL 2021, EMNLP 2022 and ICML 2023æ¥æ”¶ã€‚
  * [mPLUG](https://aclanthology.org/2022.emnlp-main.488/) é¦–æ¬¡åœ¨[VQA Challenge](https://eval.ai/web/challenges/challenge-page/830/leaderboard/2278)ä¸Šè¶…è¶Šäººç±»ã€‚
* å³å°†å‘å¸ƒ
  - [x] å¤šè¯­è¨€æ”¯æŒã€‚
  - [ ] åœ¨å¤šå›¾ç‰‡/è§†é¢‘æ•°æ®ä¸Šè®­ç»ƒçš„æ¨¡å‹
  - [X] åœ¨HuggingFace Hubä¸Šå‘å¸ƒã€‚
  - [x] Huggingface åœ¨çº¿Demo
  - [x] æŒ‡ä»¤å¾®è°ƒä»£ç ã€‚
  - [x] è§†è§‰ç›¸å…³æŒ‡ä»¤çš„æµ‹è¯„é›†**OwlEval**

## ä¸v0ç‰ˆæœ¬çš„å…¼å®¹æ€§
æˆ‘ä»¬åœ¨mainåˆ†æ”¯ä¸Šå°†ä»£ç ç”¨huggingfaceçš„ä½¿ç”¨é£æ ¼è¿›è¡Œäº†å®Œæ•´é‡æ„ï¼ŒåŒæ—¶å¯¹æ¨¡å‹ä¸­çš„ä¸€äº›é”™è¯¯è¿›è¡Œäº†ä¿®å¤å¹¶é‡æ–°è®­ç»ƒï¼Œå› æ­¤æ–°çš„checkpointå’Œæ—§çš„ä»£ç æ˜¯ä¸å…¼å®¹çš„ã€‚æˆ‘ä»¬å·²ç»å°†æ—§çš„ä»£ç ç§»åˆ°v0åˆ†æ”¯ï¼Œä½ å¯ä»¥åˆ‡æ¢åˆ°v0åˆ†æ”¯ä»¥ä½¿ç”¨å…ˆå‰çš„checkpointsã€‚

![Training paradigm and model overview](assets/model.png "Training paradigm and model overview")

## åœ¨çº¿Demo
### ModelScope
<a href="https://www.modelscope.cn/studios/damo/mPLUG-Owl/summary"><img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="250"/></a>

### Hugging Face
<!-- [![Demo of mPLUG-Owl on Modelscope](assets/modelscopeIcon.svg)](https://www.modelscope.cn/studios/damo/mPLUG-Owl/summary) -->

[![Open in Spaces](https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-xl-dark.svg)](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl)
![](assets/modelscope.png)
## æ¨¡å‹æƒé‡ Huggingface Model Hub
|Model|Phase|Download link|
|-|-|-|
|mPLUG-Owl 7B|Pre-training|[ä¸‹è½½é“¾æ¥](https://huggingface.co/MAGAer13/mplug-owl-llama-7b-pt)|
|mPLUG-Owl 7B|Instruction tuning (LoRA)|[ä¸‹è½½é“¾æ¥](https://huggingface.co/MAGAer13/mplug-owl-llama-7b)|
|mPLUG-Owl 7B|Instruction tuning (FT)|[ä¸‹è½½é“¾æ¥](https://huggingface.co/MAGAer13/mplug-owl-llama-7b-ft)|
|mPLUG-Owl 7B (å¤šè¯­è¨€ç‰ˆ)|Instruction tuning (LoRA)|[Download link](https://huggingface.co/MAGAer13/mplug-owl-bloomz-7b-multilingual)|

## OwlEval
æˆ‘ä»¬æ‰€ä½¿ç”¨çš„è¯„æµ‹é›†æ”¾åœ¨ [```./OwlEval```](OwlEval/OwlEval.md) ä¸­ã€‚

## ä½¿ç”¨
### å®‰è£…ä¾èµ–
1. åˆ›å»ºcondaç¯å¢ƒ
```bash
conda create -n mplug_owl python=3.10
conda activate mplug_owl
```

2. å®‰è£…PyTorch

```
conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia
```

3. å®‰è£…å…¶å®ƒä¾èµ–
```bash
pip install -r requirements.txt
```

### æœ¬åœ°éƒ¨ç½²Demo
æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ˜“æ‰©å±•çš„è„šæœ¬æ¥ä¸€é”®éƒ¨ç½²æœ¬åœ°Demoï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚è¿›è¡Œä¿®æ”¹ã€‚
```Bash
python -m serve.web_server --base-model 'your checkpoint directory' --bf16
```
### æ¨ç†
å¦‚æœè¦å®ç°è‡ªå®šä¹‰çš„æ¨ç†ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹æ­¥éª¤ã€‚

æ„å»ºmodel, tokenizer, processor
```Python
from pipeline.interface import get_model
model, tokenizer, processor = get_model(pretrained_ckpt='your checkpoint directory', use_bf16='use bf16 or not')
```
å‡†å¤‡æ¨¡å‹è¾“å…¥
```Python
# We use a human/AI template to organize the context as a multi-turn conversation.
# <image> denotes an image placehold.
prompts = [
'''The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: <image>
Human: Explain why this meme is funny.
AI: ''']

# The image paths should be placed in the image_list and kept in the same order as in the prompts.
# We support urls, local file paths and base64 string. You can custom the pre-process of images by modifying the mplug_owl.modeling_mplug_owl.ImageProcessor
image_list = ['https://xxx.com/image.jpg',]
```

å¯¹äºå¤šå¼ å›¾ç‰‡ï¼Œç”±äºè¿™æ˜¯æ¨¡å‹æ¶Œç°çš„èƒ½åŠ›ï¼ˆè®­ç»ƒæ•°æ®å¹¶ä¸åŒ…å«å¤šå›¾çš„æ ·æœ¬ï¼‰ï¼Œæˆ‘ä»¬å¹¶ä¸çŸ¥é“ä½•ç§æ ¼å¼å¯¹æ¨¡å‹è€Œè¨€æ˜¯æœ€ä½³çš„ã€‚ä¸‹é¢æ˜¯æˆ‘ä»¬åœ¨å®éªŒä¸­æ‰€ä½¿ç”¨çš„æ ¼å¼ã€‚å¤§å®¶ä¹Ÿå¯ä»¥è‡ªè¡Œæ¢ç´¢èƒ½è®©æ¨¡å‹æ›´å¥½ç†è§£å¤šå›¾å…³ç³»çš„æ ¼å¼æˆ–è€…promptã€‚
```Python
prompts = [
'''The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: <image>
Human: <image>
Human: Do the shirts worn by the individuals in the first and second pictures vary in color? If so, what is the specific color of each shirt?
AI: ''']

image_list = ['https://xxx.com/image_1.jpg', 'https://xxx.com/image_2.jpg']
```

è·å–æ¨¡å‹å›å¤
```Python
# generate kwargs (the same in transformers) can be passed in the do_generate()
from pipeline.interface import do_generate
sentence = do_generate(prompts, image_list, model, tokenizer, processor, 
                       use_bf16=True, max_length=512, top_k=5, do_sample=True)
```
### æŒ‡ä»¤å¾®è°ƒ
è®­ç»ƒæ ·æœ¬å­˜æ”¾åœ¨```xxx.jsonl``` ä¸­ï¼Œæ ¼å¼å¦‚ä¸‹:
```json
{"text": "The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\nHuman: Construct a sentence using the given verb in the past tense\nshudder\nAI: She shuddered at the thought of being alone in the dark.", "task_type": "gpt4instruct_sft"}
{"image": ["xxx.jpg"], "text": "The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\nHuman: <image>\nHuman: Where is the man located in the image?\nAI: The man is located in a small restaurant, sitting and enjoying his meal.\nHuman: What type of food is the man eating in the image?\nAI: The man is eating a burrito in the image.\nHuman: What can be seen next to the man in the restaurant?\nAI: Next to the man in the restaurant, there is a fridge filled with cans of soda.\nHuman: How does the man feel as he is about to eat his meal?\nAI: The man looks happy and is smiling as he is about to take a bite of his meal.\nHuman: What unique aspect of the photo makes the scene appear distorted or unusual?\nAI: The unique aspect of this photo is that it was taken using a fish-eye lens, which creates a distorted, wide-angle view of the scene, making it appear different from a standard perspective.", "task_type": "llava_sft"}
```
å…¶ä¸­ ```task_type``` å¯ä»¥ä¸º ```{'quora_chat_sft', 'sharegpt_chat_sft', 'llava_sft', 'gpt4instruct_sft'}```ã€‚

å‡†å¤‡å¥½ train.jsonl å’Œ dev.jsonl å¹¶ä¿®æ”¹ ```configs/v0.yaml``` ä¸­çš„ ```data_files```.

æ‰§è¡Œè®­ç»ƒè„šæœ¬ã€‚
```
PYTHONPATH=./ bash train_it.sh # å¦‚æœä½ æƒ³è®­ç»ƒæ•´ä¸ªLLMï¼Œæ›¿æ¢ä¸º train_it_wo_lora.sh
```
## æ€§èƒ½æ¯”è¾ƒ
æˆ‘ä»¬å±•ç¤ºäº†50ä¸ªå•è½®å¯¹è¯ï¼ˆå·¦ï¼‰å’Œ52ä¸ªå¤šè½®å¯¹è¯ï¼ˆå³ï¼‰åœ¨äººå·¥è¯„ä¼°æŒ‡æ ‡ä¸‹ï¼ŒmPLUG-Owlå’ŒåŸºçº¿æ–¹æ³•çš„æ¯”è¾ƒç»“æœã€‚A/B/C/Dè¡¨ç¤ºè¯„åˆ†äººå‘˜å¯¹æ¯ä¸ªå›å¤çš„è¯„çº§ã€‚

![Comparison Results](assets/mPLUG_Owl_compare_result_s&mturn.png)

## ç›¸å…³é¡¹ç›®

* [LLaMA](https://github.com/facebookresearch/llama). å¼€æºçš„å¤§å‹é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç³»åˆ—ã€‚
* [Baize](https://github.com/project-baize/baize-chatbot). ä½¿ç”¨LoRAåœ¨10ä¸‡ä¸ªé€šè¿‡è®©ChatGPTè‡ªèŠç”Ÿæˆçš„å¯¹è¯ä¸Šè¿›è¡Œè®­ç»ƒçš„å¼€æºèŠå¤©æ¨¡å‹ã€‚
* [Alpaca](https://github.com/tatsu-lab/stanford_alpaca). ä»7B LLaMAæ¨¡å‹ä¸Šè¿›è¡Œå¾®è°ƒè®­ç»ƒçš„ï¼Œç”¨äº52Kä¸ªæŒ‡ä»¤æ•°æ®çš„æ¨¡å‹ã€‚
* [LoRA](https://github.com/microsoft/LoRA). å³æ’å³ç”¨çš„æ¨¡å—ï¼Œå¯ä»¥æå¤§åœ°å‡å°‘ä¸‹æ¸¸ä»»åŠ¡çš„å¯è®­ç»ƒå‚æ•°æ•°é‡ã€‚
* [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4). ä»…ä½¿ç”¨ä¸€ä¸ªçº¿æ€§æ˜ å°„å±‚ï¼Œå°†å†»ç»“çš„è¯­è¨€æ¨¡å‹å’Œè§†è§‰ç¼–ç å™¨ç»“åˆçš„å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ã€‚
* [LLaVA](https://github.com/haotian-liu/LLaVA). ç»è¿‡è§†è§‰æŒ‡ä»¤è°ƒæ•´çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥å®ç°GPT4çº§åˆ«çš„èƒ½åŠ›ã€‚
* [mPLUG](https://github.com/alibaba/AliceMind/tree/main/mPLUG). è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºè·¨æ¨¡æ€ç†è§£å’Œç”Ÿæˆã€‚
* [mPLUG-2](https://github.com/alibaba/AliceMind). å…·æœ‰æ¨¡å—åŒ–è®¾è®¡çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œå¯å‘äº†æˆ‘ä»¬çš„é¡¹ç›®ã€‚

## å¼•ç”¨
å¦‚æœæˆ‘ä»¬çš„å·¥ä½œå¯¹ä½ æœ‰å¸®åŠ©ï¼Œå¯ä»¥è€ƒè™‘ç»™æˆ‘ä»¬çš„ä»“åº“ç‚¹ä¸ªstar & å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ã€‚
```
@misc{ye2023mplugowl,
      title={mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality}, 
      author={Qinghao Ye and Haiyang Xu and Guohai Xu and Jiabo Ye and Ming Yan and Yiyang Zhou and Junyang Wang and Anwen Hu and Pengcheng Shi and Yaya Shi and Chaoya Jiang and Chenliang Li and Yuanhong Xu and Hehong Chen and Junfeng Tian and Qian Qi and Ji Zhang and Fei Huang},
      year={2023},
      eprint={2304.14178},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```
